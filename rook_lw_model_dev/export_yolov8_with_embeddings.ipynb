{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# YOLOv8 Export with Embeddings\n",
    "\n",
    "Export YOLOv8n to ONNX format with multiple outputs:\n",
    "- Detection output (bounding boxes, classes, confidences)\n",
    "- Embedding output (backbone features for similarity search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-heading",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2026-02-14 18:32:04.257206712 [W:onnxruntime:Default, device_discovery.cc:131 GetPciBusId] Skipping pci_bus_id for PCI path at \"/sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/MSFT1000:00/5620e0c7-8062-4dce-aeb7-520c7ef76171\" because filename \"\"5620e0c7-8062-4dce-aeb7-520c7ef76171\"\" dit not match expected pattern of [0-9a-f]+:[0-9a-f]+:[0-9a-f]+[.][0-9a-f]+\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba519481",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b06c4128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /home/bryanc/repos/rook_lifewatch/rook_lw_model_dev/var/models\n",
      "ONNX Runtime: 1.24.1\n",
      "ONNX: 1.20.1\n",
      "PyTorch: 2.10.0+cu128\n"
     ]
    }
   ],
   "source": [
    "# Setup model directory\n",
    "model_dir = Path(\"var/models\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Model directory: {model_dir.absolute()}\")\n",
    "print(f\"ONNX Runtime: {ort.__version__}\")\n",
    "\n",
    "print(f\"ONNX: {onnx.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-heading",
   "metadata": {},
   "source": [
    "## Load YOLOv8 Nano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ YOLOv8n loaded from var/models/yolov8n.pt\n",
      "YOLOv8n summary: 129 layers, 3,157,200 parameters, 0 gradients, 8.9 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(129, 3157200, 0, 8.8575488)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = model_dir / 'yolov8n.pt'\n",
    "model = YOLO(str(model_path))\n",
    "print(f\"âœ“ YOLOv8n loaded from {model_path}\")\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arch-heading",
   "metadata": {},
   "source": [
    "## Inspect Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "inspect-arch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YOLOv8n Layers:\n",
      "shape: (23, 3)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Index â”† Type     â”† Section  â”‚\n",
      "â”‚ ---   â”† ---      â”† ---      â”‚\n",
      "â”‚ i64   â”† str      â”† str      â”‚\n",
      "â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 0     â”† Conv     â”† Backbone â”‚\n",
      "â”‚ 1     â”† Conv     â”† Backbone â”‚\n",
      "â”‚ 2     â”† C2f      â”† Backbone â”‚\n",
      "â”‚ 3     â”† Conv     â”† Backbone â”‚\n",
      "â”‚ 4     â”† C2f      â”† Backbone â”‚\n",
      "â”‚ 5     â”† Conv     â”† Backbone â”‚\n",
      "â”‚ 6     â”† C2f      â”† Backbone â”‚\n",
      "â”‚ 7     â”† Conv     â”† Backbone â”‚\n",
      "â”‚ 8     â”† C2f      â”† Backbone â”‚\n",
      "â”‚ 9     â”† SPPF     â”† Backbone â”‚\n",
      "â”‚ 10    â”† Upsample â”† Neck     â”‚\n",
      "â”‚ 11    â”† Concat   â”† Neck     â”‚\n",
      "â”‚ 12    â”† C2f      â”† Neck     â”‚\n",
      "â”‚ 13    â”† Upsample â”† Neck     â”‚\n",
      "â”‚ 14    â”† Concat   â”† Neck     â”‚\n",
      "â”‚ 15    â”† C2f      â”† Neck     â”‚\n",
      "â”‚ 16    â”† Conv     â”† Neck     â”‚\n",
      "â”‚ 17    â”† Concat   â”† Neck     â”‚\n",
      "â”‚ 18    â”† C2f      â”† Neck     â”‚\n",
      "â”‚ 19    â”† Conv     â”† Neck     â”‚\n",
      "â”‚ 20    â”† Concat   â”† Neck     â”‚\n",
      "â”‚ 21    â”† C2f      â”† Neck     â”‚\n",
      "â”‚ 22    â”† Detect   â”† Head     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ“ Embedding Extraction Point:\n",
      "   Layer 9 (SPPF) = End of backbone, best for embeddings\n",
      "   - Rich semantic features (~512 channels)\n",
      "   - Global context via spatial pyramid pooling\n",
      "   - Before multi-scale fusion (neck) begins\n",
      "\n",
      "   Architecture breakdown:\n",
      "   - Backbone: Layers 0-9\n",
      "   - Neck: Layers 10-21\n",
      "   - Head: Layer 22\n"
     ]
    }
   ],
   "source": [
    "pt_model = model.model\n",
    "\n",
    "# Find key architectural boundaries\n",
    "# First pass: find architectural boundaries\n",
    "sppf_layer_idx = None\n",
    "first_upsample_idx = None\n",
    "\n",
    "for i, module in enumerate(pt_model.model):\n",
    "    layer_type = module.__class__.__name__\n",
    "    \n",
    "    if layer_type == \"SPPF\":\n",
    "        sppf_layer_idx = i\n",
    "    \n",
    "    if layer_type == \"Upsample\" and first_upsample_idx is None:\n",
    "        first_upsample_idx = i\n",
    "\n",
    "# Second pass: build data with section labels\n",
    "layers_data = []\n",
    "for i, module in enumerate(pt_model.model):\n",
    "    layer_type = module.__class__.__name__\n",
    "    \n",
    "    # Determine section based on discovered boundaries\n",
    "    if i <= sppf_layer_idx:\n",
    "        section = \"Backbone\"\n",
    "    elif layer_type == \"Detect\":\n",
    "        section = \"Head\"\n",
    "    else:\n",
    "        section = \"Neck\"\n",
    "    \n",
    "    layers_data.append({\"Index\": i, \"Type\": layer_type, \"Section\": section})\n",
    "\n",
    "df = pl.DataFrame(layers_data)\n",
    "print(\"\\nYOLOv8n Layers:\")\n",
    "with pl.Config(tbl_rows=-1):  # Show all rows (-1), or use a number like 50\n",
    "    print(df)\n",
    "\n",
    "print(f\"\\nğŸ“ Embedding Extraction Point:\")\n",
    "print(f\"   Layer {sppf_layer_idx} (SPPF) = End of backbone, best for embeddings\")\n",
    "print(f\"   - Rich semantic features (~512 channels)\")\n",
    "print(f\"   - Global context via spatial pyramid pooling\")\n",
    "print(f\"   - Before multi-scale fusion (neck) begins\")\n",
    "print(f\"\\n   Architecture breakdown:\")\n",
    "print(f\"   - Backbone: Layers 0-{sppf_layer_idx}\")\n",
    "print(f\"   - Neck: Layers {first_upsample_idx}-{len(pt_model.model)-2}\")\n",
    "print(f\"   - Head: Layer {len(pt_model.model)-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapper-heading",
   "metadata": {},
   "source": [
    "## Create Multi-Output Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wrapper-class",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings from layer 9 (SPPF)\n",
      "âœ“ Wrapper created\n"
     ]
    }
   ],
   "source": [
    "class YOLOv8WithEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper that extracts both detections and embeddings from YOLOv8.\n",
    "    \n",
    "    Architecture:\n",
    "    - Backbone: Feature extraction, ends with SPPF layer\n",
    "    - Neck: Multi-scale feature fusion (starts with Upsample)\n",
    "    - Head: Detection output (Detect layer)\n",
    "    \n",
    "    This wrapper captures backbone features at the SPPF layer which provides:\n",
    "    - High-level semantic representations (~512 channels)\n",
    "    - Global spatial context via pyramid pooling\n",
    "    - Pre-fusion features ideal for similarity comparison\n",
    "    \"\"\"\n",
    "    def __init__(self, yolo_model, embedding_layer_idx=None):\n",
    "        super().__init__()\n",
    "        # Get the underlying DetectionModel from YOLO wrapper\n",
    "        self.model = yolo_model.model\n",
    "        \n",
    "        # Auto-detect embedding layer if not specified\n",
    "        if embedding_layer_idx is None:\n",
    "            # Find SPPF layer (end of backbone)\n",
    "            for i, layer in enumerate(self.model.model):\n",
    "                if layer.__class__.__name__ == \"SPPF\":\n",
    "                    embedding_layer_idx = i\n",
    "                    break\n",
    "            \n",
    "            if embedding_layer_idx is None:\n",
    "                raise RuntimeError(\"Could not find SPPF layer in model architecture\")\n",
    "        \n",
    "        self.embedding_layer_idx = embedding_layer_idx\n",
    "        self.embedding_layer = self.model.model[self.embedding_layer_idx]\n",
    "        self._embeddings = None\n",
    "        \n",
    "        # Register hook to capture SPPF output\n",
    "        self.embedding_layer.register_forward_hook(self._hook_fn)\n",
    "        \n",
    "        print(f\"Extracting embeddings from layer {self.embedding_layer_idx} ({self.embedding_layer.__class__.__name__})\")\n",
    "    \n",
    "    def _hook_fn(self, module, input, output):\n",
    "        \"\"\"Hook function to capture SPPF layer output\"\"\"\n",
    "        # Apply global average pooling: [Batch, Channels, Height, Width] -> [Batch, Channels]\n",
    "        self._embeddings = torch.nn.functional.adaptive_avg_pool2d(output, (1, 1)).squeeze(-1).squeeze(-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reset embeddings\n",
    "        self._embeddings = None\n",
    "        \n",
    "        # Run normal YOLOv8 forward pass (hook will capture SPPF output)\n",
    "        detections = self.model(x)\n",
    "        \n",
    "        # YOLOv8 may return tuple/list of outputs, extract the main detection tensor\n",
    "        if isinstance(detections, (list, tuple)):\n",
    "            detections = detections[0]\n",
    "        \n",
    "        if self._embeddings is None:\n",
    "            raise RuntimeError(f\"Failed to capture embeddings from layer {self.embedding_layer_idx}\")\n",
    "        \n",
    "        # detections: [1, 84, 8400] - (4 bbox coords + 80 class scores, 8400 predictions)\n",
    "        # embeddings: [1, channels] - feature vector for similarity search\n",
    "        return detections, self._embeddings\n",
    "\n",
    "# Create wrapper - automatically finds SPPF layer\n",
    "wrapped_model = YOLOv8WithEmbeddings(model)\n",
    "wrapped_model.eval()\n",
    "print(\"âœ“ Wrapper created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-heading",
   "metadata": {},
   "source": [
    "## Test Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "test-wrapper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detections: torch.Size([1, 84, 8400])\n",
      "Embeddings: torch.Size([1, 256])\n",
      "âœ“ 256-dim feature vector\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 640, 640)\n",
    "\n",
    "with torch.no_grad():\n",
    "    detections, embeddings = wrapped_model(dummy_input)\n",
    "\n",
    "print(f\"Detections: {detections.shape}\")\n",
    "print(f\"Embeddings: {embeddings.shape}\")\n",
    "print(f\"âœ“ {embeddings.shape[1]}-dim feature vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-heading",
   "metadata": {},
   "source": [
    "## Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-onnx",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100566/4001159241.py:3: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W0214 18:32:04.586000 100566 torch/onnx/_internal/exporter/_compat.py:125] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "W0214 18:32:05.012000 100566 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
      "W0214 18:32:05.013000 100566 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
      "W0214 18:32:05.014000 100566 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n",
      "W0214 18:32:05.015000 100566 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `YOLOv8WithEmbeddings([...]` with `torch.export.export(..., strict=False)`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/contextlib.py:148: UserWarning: The tensor attribute self._embeddings was assigned during export. Such attributes must be registered as buffers using the `register_buffer` API (https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer).\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `YOLOv8WithEmbeddings([...]` with `torch.export.export(..., strict=False)`... âœ…\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/copyreg.py:99: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "  return cls.__new__(cls, *args)\n",
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n",
      "Failed to convert the model to the target version 17 using the ONNX C API. The model was not modified\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bryanc/repos/rook_lifewatch/rook_lw_model_dev/.venv/lib/python3.13/site-packages/onnxscript/version_converter/__init__.py\", line 120, in call\n",
      "    converted_proto = _c_api_utils.call_onnx_api(\n",
      "        func=_partial_convert_version, model=model\n",
      "    )\n",
      "  File \"/home/bryanc/repos/rook_lifewatch/rook_lw_model_dev/.venv/lib/python3.13/site-packages/onnxscript/version_converter/_c_api_utils.py\", line 65, in call_onnx_api\n",
      "    result = func(proto)\n",
      "  File \"/home/bryanc/repos/rook_lifewatch/rook_lw_model_dev/.venv/lib/python3.13/site-packages/onnxscript/version_converter/__init__.py\", line 115, in _partial_convert_version\n",
      "    return onnx.version_converter.convert_version(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        proto, target_version=self.target_version\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/bryanc/repos/rook_lifewatch/rook_lw_model_dev/.venv/lib/python3.13/site-packages/onnx/version_converter.py\", line 39, in convert_version\n",
      "    converted_model_str = C.convert_version(model_str, target_version)\n",
      "RuntimeError: /github/workspace/onnx/version_converter/adapters/axes_input_to_attribute.h:65: adapt: Assertion `node->hasAttribute(kaxes)` failed: No initializer or constant input to node found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... âœ…\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... âœ…\n",
      "Applied 123 of general pattern rewrite rules.\n",
      "âœ“ Exported: /home/bryanc/repos/rook_lifewatch/rook_lw_model_dev/var/models/yolov8n_with_embeddings.onnx\n",
      "  Size: 0.49 MB\n"
     ]
    }
   ],
   "source": [
    "output_path = model_dir / \"yolov8n_with_embeddings.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    wrapped_model,\n",
    "    dummy_input,\n",
    "    output_path,\n",
    "    export_params=True,\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['images'],\n",
    "    output_names=['detections', 'embeddings'],\n",
    "    dynamic_axes={\n",
    "        'images': {0: 'batch'},\n",
    "        'detections': {0: 'batch'},\n",
    "        'embeddings': {0: 'batch'}\n",
    "    },\n",
    "    # Keep all data in single file (no external .data file)\n",
    "    dynamo=False\n",
    ")\n",
    "\n",
    "size_mb = output_path.stat().st_size / 1024 / 1024\n",
    "print(f\"âœ“ Exported: {output_path.absolute()}\")\n",
    "print(f\"  Size: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-heading",
   "metadata": {},
   "source": [
    "## Verify ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "verify-onnx",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ONNX model valid\n",
      "\n",
      "Inputs:\n",
      "  images\n",
      "\n",
      "Outputs:\n",
      "  detections\n",
      "  embeddings\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(str(output_path))\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"âœ“ ONNX model valid\")\n",
    "\n",
    "print(\"\\nInputs:\")\n",
    "for inp in onnx_model.graph.input:\n",
    "    print(f\"  {inp.name}\")\n",
    "\n",
    "print(\"\\nOutputs:\")\n",
    "for out in onnx_model.graph.output:\n",
    "    print(f\"  {out.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-ort-heading",
   "metadata": {},
   "source": [
    "## Test ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "test-ort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ONNX Runtime inference successful\n",
      "Detections: (1, 84, 8400)\n",
      "Embeddings: (1, 256)\n",
      "\n",
      "Max diff from PyTorch:\n",
      "  Detections: 0.001373\n",
      "  Embeddings: 0.000002\n"
     ]
    }
   ],
   "source": [
    "session = ort.InferenceSession(str(output_path))\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_names = [o.name for o in session.get_outputs()]\n",
    "\n",
    "outputs = session.run(output_names, {input_name: dummy_input.numpy()})\n",
    "det_onnx, emb_onnx = outputs\n",
    "\n",
    "print(f\"âœ“ ONNX Runtime inference successful\")\n",
    "print(f\"Detections: {det_onnx.shape}\")\n",
    "print(f\"Embeddings: {emb_onnx.shape}\")\n",
    "\n",
    "# Compare with PyTorch\n",
    "with torch.no_grad():\n",
    "    det_pt, emb_pt = wrapped_model(dummy_input)\n",
    "\n",
    "det_diff = np.abs(det_onnx - det_pt.numpy()).max()\n",
    "emb_diff = np.abs(emb_onnx - emb_pt.numpy()).max()\n",
    "\n",
    "print(f\"\\nMax diff from PyTorch:\")\n",
    "print(f\"  Detections: {det_diff:.6f}\")\n",
    "print(f\"  Embeddings: {emb_diff:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-heading",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Model exported with dual outputs:\n",
    "- `detections`: [1, 84, 8400] - Standard YOLOv8 format\n",
    "- `embeddings`: [1, channels] - Feature vector\n",
    "\n",
    "Ready for use in Rust/ONNX Runtime!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rook_lw_model_dev (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
