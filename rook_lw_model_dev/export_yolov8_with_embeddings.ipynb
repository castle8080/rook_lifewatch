{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# YOLOv8 Export with Embeddings\n",
    "\n",
    "Export YOLOv8n to ONNX format with multiple outputs:\n",
    "- Detection output (bounding boxes, classes, confidences)\n",
    "- Embedding output (backbone features for similarity search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-heading",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2026-02-15 08:28:12.628183032 [W:onnxruntime:Default, device_discovery.cc:131 GetPciBusId] Skipping pci_bus_id for PCI path at \"/sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/MSFT1000:00/5620e0c7-8062-4dce-aeb7-520c7ef76171\" because filename \"\"5620e0c7-8062-4dce-aeb7-520c7ef76171\"\" dit not match expected pattern of [0-9a-f]+:[0-9a-f]+:[0-9a-f]+[.][0-9a-f]+\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba519481",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b06c4128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /home/bryanc/repos/rook_lifewatch/rook_lw_model_dev/models\n",
      "Download directory: /home/bryanc/repos/rook_lifewatch/rook_lw_model_dev/var/downloads\n",
      "ONNX Runtime: 1.24.1\n",
      "ONNX: 1.20.1\n",
      "PyTorch: 2.10.0+cu128\n"
     ]
    }
   ],
   "source": [
    "# Setup model directory\n",
    "model_dir = Path(\"models\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "download_dir = Path(\"var/downloads\")\n",
    "download_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Model directory: {model_dir.absolute()}\")\n",
    "print(f\"Download directory: {download_dir.absolute()}\")\n",
    "print(f\"ONNX Runtime: {ort.__version__}\")\n",
    "\n",
    "print(f\"ONNX: {onnx.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-heading",
   "metadata": {},
   "source": [
    "## Load YOLOv8 Nano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ YOLOv8n loaded from var/downloads/yolov8n.pt\n",
      "YOLOv8n summary: 129 layers, 3,157,200 parameters, 0 gradients, 8.9 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(129, 3157200, 0, 8.8575488)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = download_dir / 'yolov8n.pt'\n",
    "model = YOLO(str(model_path))\n",
    "print(f\"âœ“ YOLOv8n loaded from {model_path}\")\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arch-heading",
   "metadata": {},
   "source": [
    "## Inspect Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "inspect-arch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YOLOv8n Layers:\n",
      "shape: (23, 3)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Index â”† Type     â”† Section  â”‚\n",
      "â”‚ ---   â”† ---      â”† ---      â”‚\n",
      "â”‚ i64   â”† str      â”† str      â”‚\n",
      "â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 0     â”† Conv     â”† Backbone â”‚\n",
      "â”‚ 1     â”† Conv     â”† Backbone â”‚\n",
      "â”‚ 2     â”† C2f      â”† Backbone â”‚\n",
      "â”‚ 3     â”† Conv     â”† Backbone â”‚\n",
      "â”‚ 4     â”† C2f      â”† Backbone â”‚\n",
      "â”‚ 5     â”† Conv     â”† Backbone â”‚\n",
      "â”‚ 6     â”† C2f      â”† Backbone â”‚\n",
      "â”‚ 7     â”† Conv     â”† Backbone â”‚\n",
      "â”‚ 8     â”† C2f      â”† Backbone â”‚\n",
      "â”‚ 9     â”† SPPF     â”† Backbone â”‚\n",
      "â”‚ 10    â”† Upsample â”† Neck     â”‚\n",
      "â”‚ 11    â”† Concat   â”† Neck     â”‚\n",
      "â”‚ 12    â”† C2f      â”† Neck     â”‚\n",
      "â”‚ 13    â”† Upsample â”† Neck     â”‚\n",
      "â”‚ 14    â”† Concat   â”† Neck     â”‚\n",
      "â”‚ 15    â”† C2f      â”† Neck     â”‚\n",
      "â”‚ 16    â”† Conv     â”† Neck     â”‚\n",
      "â”‚ 17    â”† Concat   â”† Neck     â”‚\n",
      "â”‚ 18    â”† C2f      â”† Neck     â”‚\n",
      "â”‚ 19    â”† Conv     â”† Neck     â”‚\n",
      "â”‚ 20    â”† Concat   â”† Neck     â”‚\n",
      "â”‚ 21    â”† C2f      â”† Neck     â”‚\n",
      "â”‚ 22    â”† Detect   â”† Head     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ“ Embedding Extraction Point:\n",
      "   Layer 9 (SPPF) = End of backbone, best for embeddings\n",
      "   - Rich semantic features (~512 channels)\n",
      "   - Global context via spatial pyramid pooling\n",
      "   - Before multi-scale fusion (neck) begins\n",
      "\n",
      "   Architecture breakdown:\n",
      "   - Backbone: Layers 0-9\n",
      "   - Neck: Layers 10-21\n",
      "   - Head: Layer 22\n"
     ]
    }
   ],
   "source": [
    "pt_model = model.model\n",
    "\n",
    "# Find key architectural boundaries\n",
    "# First pass: find architectural boundaries\n",
    "sppf_layer_idx = None\n",
    "first_upsample_idx = None\n",
    "\n",
    "for i, module in enumerate(pt_model.model):\n",
    "    layer_type = module.__class__.__name__\n",
    "    \n",
    "    if layer_type == \"SPPF\":\n",
    "        sppf_layer_idx = i\n",
    "    \n",
    "    if layer_type == \"Upsample\" and first_upsample_idx is None:\n",
    "        first_upsample_idx = i\n",
    "\n",
    "# Second pass: build data with section labels\n",
    "layers_data = []\n",
    "for i, module in enumerate(pt_model.model):\n",
    "    layer_type = module.__class__.__name__\n",
    "    \n",
    "    # Determine section based on discovered boundaries\n",
    "    if i <= sppf_layer_idx:\n",
    "        section = \"Backbone\"\n",
    "    elif layer_type == \"Detect\":\n",
    "        section = \"Head\"\n",
    "    else:\n",
    "        section = \"Neck\"\n",
    "    \n",
    "    layers_data.append({\"Index\": i, \"Type\": layer_type, \"Section\": section})\n",
    "\n",
    "df = pl.DataFrame(layers_data)\n",
    "print(\"\\nYOLOv8n Layers:\")\n",
    "with pl.Config(tbl_rows=-1):  # Show all rows (-1), or use a number like 50\n",
    "    print(df)\n",
    "\n",
    "print(f\"\\nğŸ“ Embedding Extraction Point:\")\n",
    "print(f\"   Layer {sppf_layer_idx} (SPPF) = End of backbone, best for embeddings\")\n",
    "print(f\"   - Rich semantic features (~512 channels)\")\n",
    "print(f\"   - Global context via spatial pyramid pooling\")\n",
    "print(f\"   - Before multi-scale fusion (neck) begins\")\n",
    "print(f\"\\n   Architecture breakdown:\")\n",
    "print(f\"   - Backbone: Layers 0-{sppf_layer_idx}\")\n",
    "print(f\"   - Neck: Layers {first_upsample_idx}-{len(pt_model.model)-2}\")\n",
    "print(f\"   - Head: Layer {len(pt_model.model)-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapper-heading",
   "metadata": {},
   "source": [
    "## Create Multi-Output Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wrapper-class",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings from layer 9 (SPPF)\n",
      "âœ“ Wrapper created\n"
     ]
    }
   ],
   "source": [
    "class YOLOv8WithEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper that extracts both detections and embeddings from YOLOv8.\n",
    "    \n",
    "    Architecture:\n",
    "    - Backbone: Feature extraction, ends with SPPF layer\n",
    "    - Neck: Multi-scale feature fusion (starts with Upsample)\n",
    "    - Head: Detection output (Detect layer)\n",
    "    \n",
    "    This wrapper captures backbone features at the SPPF layer which provides:\n",
    "    - High-level semantic representations (~512 channels)\n",
    "    - Global spatial context via pyramid pooling\n",
    "    - Pre-fusion features ideal for similarity comparison\n",
    "    \"\"\"\n",
    "    def __init__(self, yolo_model, embedding_layer_idx=None):\n",
    "        super().__init__()\n",
    "        # Get the underlying DetectionModel from YOLO wrapper\n",
    "        self.model = yolo_model.model\n",
    "        \n",
    "        # Auto-detect embedding layer if not specified\n",
    "        if embedding_layer_idx is None:\n",
    "            # Find SPPF layer (end of backbone)\n",
    "            for i, layer in enumerate(self.model.model):\n",
    "                if layer.__class__.__name__ == \"SPPF\":\n",
    "                    embedding_layer_idx = i\n",
    "                    break\n",
    "            \n",
    "            if embedding_layer_idx is None:\n",
    "                raise RuntimeError(\"Could not find SPPF layer in model architecture\")\n",
    "        \n",
    "        self.embedding_layer_idx = embedding_layer_idx\n",
    "        self.embedding_layer = self.model.model[self.embedding_layer_idx]\n",
    "        self._embeddings = None\n",
    "        \n",
    "        # Register hook to capture SPPF output\n",
    "        self.embedding_layer.register_forward_hook(self._hook_fn)\n",
    "        \n",
    "        print(f\"Extracting embeddings from layer {self.embedding_layer_idx} ({self.embedding_layer.__class__.__name__})\")\n",
    "    \n",
    "    def _hook_fn(self, module, input, output):\n",
    "        \"\"\"Hook function to capture SPPF layer output\"\"\"\n",
    "        # Apply global average pooling: [Batch, Channels, Height, Width] -> [Batch, Channels]\n",
    "        self._embeddings = torch.nn.functional.adaptive_avg_pool2d(output, (1, 1)).squeeze(-1).squeeze(-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reset embeddings\n",
    "        self._embeddings = None\n",
    "        \n",
    "        # Run normal YOLOv8 forward pass (hook will capture SPPF output)\n",
    "        detections = self.model(x)\n",
    "        \n",
    "        # YOLOv8 may return tuple/list of outputs, extract the main detection tensor\n",
    "        if isinstance(detections, (list, tuple)):\n",
    "            detections = detections[0]\n",
    "        \n",
    "        if self._embeddings is None:\n",
    "            raise RuntimeError(f\"Failed to capture embeddings from layer {self.embedding_layer_idx}\")\n",
    "        \n",
    "        # detections: [1, 84, 8400] - (4 bbox coords + 80 class scores, 8400 predictions)\n",
    "        # embeddings: [1, channels] - feature vector for similarity search\n",
    "        return detections, self._embeddings\n",
    "\n",
    "# Create wrapper - automatically finds SPPF layer\n",
    "wrapped_model = YOLOv8WithEmbeddings(model)\n",
    "wrapped_model.eval()\n",
    "print(\"âœ“ Wrapper created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-heading",
   "metadata": {},
   "source": [
    "## Test Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "test-wrapper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detections: torch.Size([1, 84, 8400])\n",
      "Embeddings: torch.Size([1, 256])\n",
      "âœ“ 256-dim feature vector\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 640, 640)\n",
    "\n",
    "with torch.no_grad():\n",
    "    detections, embeddings = wrapped_model(dummy_input)\n",
    "\n",
    "print(f\"Detections: {detections.shape}\")\n",
    "print(f\"Embeddings: {embeddings.shape}\")\n",
    "print(f\"âœ“ {embeddings.shape[1]}-dim feature vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-heading",
   "metadata": {},
   "source": [
    "## Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "export-onnx",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129206/1890606220.py:3: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter has become the default. Learn more about the new export logic: https://docs.pytorch.org/docs/stable/onnx_export.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Exported: /home/bryanc/repos/rook_lifewatch/rook_lw_model_dev/models/yolov8n_with_embeddings.onnx\n",
      "  Size: 12.25 MB\n"
     ]
    }
   ],
   "source": [
    "output_path = model_dir / \"yolov8n_with_embeddings.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    wrapped_model,\n",
    "    dummy_input,\n",
    "    output_path,\n",
    "    export_params=True,\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['images'],\n",
    "    output_names=['detections', 'embeddings'],\n",
    "    dynamic_axes={\n",
    "        'images': {0: 'batch'},\n",
    "        'detections': {0: 'batch'},\n",
    "        'embeddings': {0: 'batch'}\n",
    "    },\n",
    "    # Keep all data in single file (no external .data file)\n",
    "    dynamo=False\n",
    ")\n",
    "\n",
    "size_mb = output_path.stat().st_size / 1024 / 1024\n",
    "print(f\"âœ“ Exported: {output_path.absolute()}\")\n",
    "print(f\"  Size: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-heading",
   "metadata": {},
   "source": [
    "## Verify ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "verify-onnx",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ONNX model valid\n",
      "\n",
      "Inputs:\n",
      "  images\n",
      "\n",
      "Outputs:\n",
      "  detections\n",
      "  embeddings\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(str(output_path))\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"âœ“ ONNX model valid\")\n",
    "\n",
    "print(\"\\nInputs:\")\n",
    "for inp in onnx_model.graph.input:\n",
    "    print(f\"  {inp.name}\")\n",
    "\n",
    "print(\"\\nOutputs:\")\n",
    "for out in onnx_model.graph.output:\n",
    "    print(f\"  {out.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-ort-heading",
   "metadata": {},
   "source": [
    "## Test ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "test-ort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ONNX Runtime inference successful\n",
      "Detections: (1, 84, 8400)\n",
      "Embeddings: (1, 256)\n",
      "\n",
      "Max diff from PyTorch:\n",
      "  Detections: 0.000977\n",
      "  Embeddings: 0.000003\n"
     ]
    }
   ],
   "source": [
    "session = ort.InferenceSession(str(output_path))\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_names = [o.name for o in session.get_outputs()]\n",
    "\n",
    "outputs = session.run(output_names, {input_name: dummy_input.numpy()})\n",
    "det_onnx, emb_onnx = outputs\n",
    "\n",
    "print(f\"âœ“ ONNX Runtime inference successful\")\n",
    "print(f\"Detections: {det_onnx.shape}\")\n",
    "print(f\"Embeddings: {emb_onnx.shape}\")\n",
    "\n",
    "# Compare with PyTorch\n",
    "with torch.no_grad():\n",
    "    det_pt, emb_pt = wrapped_model(dummy_input)\n",
    "\n",
    "det_diff = np.abs(det_onnx - det_pt.numpy()).max()\n",
    "emb_diff = np.abs(emb_onnx - emb_pt.numpy()).max()\n",
    "\n",
    "print(f\"\\nMax diff from PyTorch:\")\n",
    "print(f\"  Detections: {det_diff:.6f}\")\n",
    "print(f\"  Embeddings: {emb_diff:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-heading",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Model exported with dual outputs:\n",
    "- `detections`: [1, 84, 8400] - Standard YOLOv8 format\n",
    "- `embeddings`: [1, channels] - Feature vector\n",
    "\n",
    "Ready for use in Rust/ONNX Runtime!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
